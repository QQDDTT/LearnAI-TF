# ============================================================
# 通用深度学习训练配置框架
# 支持：监督、无监督、强化、自监督、半监督、多任务等所有方式
# 作者：AI Framework Team
# 版本：1.0
# 最后更新：2025年10月
# ============================================================

# ========== 第一层：全局配置 ==========
# 作用：定义训练任务的全局参数
# 说明：这些参数对整个训练过程有效
global:
  name: "universal_training_framework"      # 项目名称
  version: "1.0"                            # 配置版本
  seed: 42                                  # 随机种子（保证可复现性）

# ========== 第二层：训练方式声明 ==========
# 作用：这是关键中的关键！决定了使用哪种训练流程
# 说明：Python主循环会根据这个字段选择对应的 training_pipeline
# 重要：不同的训练方式有完全不同的数据流和反馈机制
training_mode:
  # type: 训练方式的类型，决定了整个流程
  # 可选值：
  #   - supervised        : 监督学习（有标签的离线数据）
  #   - unsupervised      : 无监督学习（无标签，如聚类）
  #   - reinforcement     : 强化学习（交互式，基于奖励）
  #   - self_supervised   : 自监督学习（无标签但自生成目标）
  #   - semi_supervised   : 半监督学习（部分标签）
  #   - multi_task        : 多任务学习（同时学习多个相关任务）
  #   - transfer          : 迁移学习（预训练+微调）
  #   - online            : 在线学习（流式数据）
  #   - active            : 主动学习（模型选择标注样本）
  #   - adversarial       : 对抗训练（如GAN）
  #   - curriculum        : 课程学习（从易到难）
  type: "supervised"

  # subtype: 训练方式的细分
  # 例如强化学习的细分：
  #   - q_learning        : Q-learning方法
  #   - policy_gradient   : 策略梯度方法
  #   - actor_critic      : 行动者-评论家方法
  # 监督学习通常为null
  subtype: null

# ========== 第三层：模型定义 ==========
# 作用：定义所有需要的神经网络模型
# 说明：与训练方式无关，任何训练方式都可以用深度网络或浅层网络
# 关键：使用 reflection 字段通过反射调用构造函数，支持动态构建
models:
  # 示例1：监督学习的单一模型
  # 用于生成器（生成假图片）
  generator:
    type: "Functional"                        # 模型类型
    reflection: tensorflow.keras.Sequential   # 构造函数路径（反射调用）
    layers:
      # 第一层：全连接层
      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 128                          # 神经元数
          activation: relu                    # 激活函数
          input_shape: [100]                  # 输入维度（仅第一层需要）

      # 第二层：全连接层
      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 784                          # 输出维度（28x28=784像素）
          activation: tanh                    # 输出层用tanh（范围[-1,1]）
          name: fake_image                    # 层命名

  # 示例2：强化学习的Actor网络（策略网络）
  # 作用：根据状态选择动作
  # 输入：游戏状态（32维）
  # 输出：动作概率分布（16维，对应16个可能的动作）
  actor:
    type: "Functional"
    reflection: tensorflow.keras.Sequential
    layers:
      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 64                           # 隐层神经元
          activation: relu
          input_shape: [32]                   # 状态空间维度

      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 16                           # 动作空间维度
          activation: softmax                 # 输出概率分布
          name: action_logits

  # 示例3：强化学习的Critic网络（价值网络）
  # 作用：评估状态的好坏
  # 输入：游戏状态（32维）
  # 输出：状态价值（1维标量）
  critic:
    type: "Functional"
    reflection: tensorflow.keras.Sequential
    layers:
      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 64
          activation: relu
          input_shape: [32]

      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 1                            # 价值标量
          name: state_value

  # 示例4：无监督学习的编码器
  # 作用：将高维数据压缩为低维表示
  # 用于：自编码器、变分自编码器、对比学习等
  encoder:
    type: "Functional"
    reflection: tensorflow.keras.Sequential
    layers:
      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 128
          activation: relu
          input_shape: [784]                  # 图片像素数（28x28）

      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 64                           # 隐空间维度
          activation: relu

  # 示例5：无监督学习的解码器
  # 作用：将低维表示解码回原始维度
  # 用于：重构数据
  decoder:
    type: "Functional"
    reflection: tensorflow.keras.Sequential
    layers:
      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 128
          activation: relu
          input_shape: [64]                   # 隐空间维度

      - reflection: tensorflow.keras.layers.Dense
        args:
          units: 784                          # 重构回原维度
          activation: tanh

# ========== 第四层：优化器定义 ==========
# 作用：定义用于更新模型参数的优化算法
# 说明：为每个模型配置独立的优化器，以支持不同的学习率
# 注意：多个模型可以共享优化器，也可以使用不同的优化器
optimizers:
  # 示例1：生成器的Adam优化器
  # 用于对抗网络的生成器
  generator_opt:
    reflection: tensorflow.keras.optimizers.Adam
    args:
      learning_rate: 0.0002                  # 学习率（GAN通常很小）
      beta_1: 0.5                            # 一阶矩估计的衰减率

  # 示例2：判别器的Adam优化器
  # 用于对抗网络的判别器
  discriminator_opt:
    reflection: tensorflow.keras.optimizers.Adam
    args:
      learning_rate: 0.0002
      beta_1: 0.5

  # 示例3：Actor网络的优化器
  # 强化学习中的策略网络
  actor_opt:
    reflection: tensorflow.keras.optimizers.Adam
    args:
      learning_rate: 0.001                   # 比GAN更大的学习率

  # 示例4：Critic网络的优化器
  # 强化学习中的价值网络
  critic_opt:
    reflection: tensorflow.keras.optimizers.Adam
    args:
      learning_rate: 0.001

  # 示例5：编码器的优化器
  # 用于无监督或自监督学习
  encoder_opt:
    reflection: tensorflow.keras.optimizers.Adam
    args:
      learning_rate: 0.001

# ========== 第五层：损失函数定义 ==========
# 作用：定义优化目标（指导模型学习的方向）
# 说明：不同的训练方式使用不同的损失函数
# 注意：损失函数可以在step_sequence中被引用
losses:
  # 监督学习的损失函数
  # 用于二元分类任务
  supervised_loss:
    reflection: tensorflow.keras.losses.BinaryCrossentropy
    args: {}

  # 无监督学习的重构损失
  # 用于自编码器，衡量重构的准确性
  reconstruction_loss:
    reflection: tensorflow.keras.losses.MeanSquaredError
    args: {}

  # 自监督学习的对比损失
  # 用于对比学习，使相似样本靠近，不相似样本远离
  contrastive_loss:
    reflection: modules.losses.ContrastiveLoss
    args:
      temperature: 0.07                      # 温度参数（控制相似度的尖锐程度）

# ========== 第六层：数据管理 ==========
# 作用：定义数据来源和数据流向
# 说明：这里是一个关键创新！
#       不同的训练方式有完全不同的数据来源方式：
#       - 监督学习：离线批量数据（train/val/test）
#       - 强化学习：交互式，每次一条数据
#       - 无监督学习：无标签数据
#       - 自监督学习：无标签+需要增强
data_manager:
  # ========== 监督学习数据源 ==========
  # 特点：有标签，离线，分为train/val/test三部分
  supervised_source:
    # 训练数据
    train:
      reflection: modules.utils.build_csv_loader
      args:
        file_path: "data/train.csv"           # 训练数据路径
        batch_size: 64                        # 批次大小
        shuffle: True                         # 是否打乱数据

    # 验证数据（用于评估和早停）
    val:
      reflection: modules.utils.build_csv_loader
      args:
        file_path: "data/val.csv"
        batch_size: 64
        shuffle: False                        # 验证集通常不打乱

    # 测试数据（最终评估）
    test:
      reflection: modules.utils.build_csv_loader
      args:
        file_path: "data/test.csv"
        batch_size: 32
        shuffle: False

  # ========== 强化学习数据源 ==========
  # 特点：交互式，每次请求一条数据，获得奖励反馈
  # 这是在线学习的典型模式
  rl_source:
    type: "interactive"                      # 标记为交互式

    # 环境定义（游戏或模拟器）
    environment:
      reflection: modules.env.GameEnvironment
      args:
        game_name: "CartPole-v1"              # 环境名称

    # 获取状态的请求
    # 这会从外部服务器获取当前游戏状态
    request:
      reflection: modules.http_client.fetch_state
      args:
        url: "http://game-server:8080/api/state"
        method: POST
        timeout: 5                            # 超时时间（秒）

    # 提交动作并获取反馈
    # 每个动作会返回：奖励、下一状态、是否结束等信息
    feedback:
      reflection: modules.http_client.submit_action
      args:
        url: "http://game-server:8080/api/step"
        method: POST

  # ========== 无监督学习数据源 ==========
  # 特点：无标签，用于聚类、降维等
  unsupervised_source:
    reflection: modules.utils.build_csv_loader
    args:
      file_path: "data/unlabeled.csv"        # 无标签数据
      batch_size: 64
      shuffle: True

  # ========== 自监督学习数据源 ==========
  # 特点：无标签，但需要数据增强生成对比对
  # 流程：原始数据 → 增强1、增强2 → 模型学习它们应该相似
  self_supervised_source:
    reflection: modules.utils.build_csv_loader
    args:
      file_path: "data/unlabeled.csv"
      batch_size: 64
      shuffle: True

    # 数据增强配置
    augmentation:
      enabled: True                           # 是否启用增强
      transforms:                            # 增强方式列表
        # 随机裁剪
        - reflection: modules.augmentation.RandomCrop
          args:
            size: [224, 224]                  # 裁剪后的大小

        # 随机翻转
        - reflection: modules.augmentation.RandomFlip
          args: {}

        # 随机颜色抖动
        - reflection: modules.augmentation.RandomColorJitter
          args:
            brightness: 0.2                   # 亮度变化范围

# ========== 第七层：训练流程定义 ==========
# 作用：定义每种训练方式的具体执行步骤
# 说明：这是框架的核心创新！
#       每种训练方式都有自己的loop_type和step_sequence
#       Python主循环会根据training_mode.type选择对应的流程
training_pipeline:

  # ========== 监督学习流程 ==========
  # 流程图：
  # for epoch in epochs:
  #   for batch in data:
  #     pred = forward(batch)
  #     loss = compute_loss(pred, label)
  #     grad = backward(loss)
  #     update(grad)
  supervised:
    # 循环类型：外层循环是epoch，内层循环是batch
    loop_type: "epoch_batch"

    parameters:
      epochs: 100                             # 训练的总轮数
      steps_per_epoch: null                   # null表示用完整数据集

    # 单个step的执行步骤（逐个反射调用）
    step_sequence:
      # 步骤1：获取一个batch的数据
      - name: "fetch_batch"
        reflection: modules.data.get_batch
        args:
          loader: "train_loader"

      # 步骤2：前向传播（模型预测）
      - name: "forward_pass"
        reflection: modules.inference.forward
        args:
          model: "generator"                  # 使用哪个模型
          inputs: "batch.x"                   # 输入数据

      # 步骤3：计算损失（预测与标签的差异）
      - name: "compute_loss"
        reflection: modules.losses.compute_supervised_loss
        args:
          predictions: "last_result"          # 上一步的结果
          targets: "batch.y"                  # 真实标签
          loss_fn: "supervised_loss"

      # 步骤4：反向传播（计算梯度）
      - name: "backward_pass"
        reflection: modules.optimizer.compute_gradients
        args:
          loss: "last_result"
          model: "generator"

      # 步骤5：更新参数（使用梯度优化权重）
      - name: "update_params"
        reflection: modules.optimizer.apply_gradients
        args:
          optimizer: "generator_opt"
          gradients: "last_result"
          model: "generator"

    # 循环条件（何时停止训练）
    loop_condition:
      check_type: "epoch_based"               # 基于epoch数量判断
      max_epochs: "config.training_pipeline.supervised.parameters.epochs"

    # 周期性评估（在验证集上评估）
    evaluation:
      frequency: "epoch"                      # 每个epoch评估一次
      eval_split: "val"                       # 使用验证集

  # ========== 强化学习流程 ==========
  # 流程图：
  # for episode in episodes:
  #   state = env.reset()
  #   for step in steps:
  #     action = policy(state)
  #     next_state, reward = env.step(action)
  #     store_experience(state, action, reward, next_state)
  #     update_policy(...)
  reinforcement:
    # 循环类型：外层循环是episode，内层循环是step
    # 这是强化学习的特有循环结构
    loop_type: "episode_step"

    parameters:
      episodes: 1000                          # 总训练episode数
      steps_per_episode: 500                  # 每个episode最多执行的step数
      buffer_size: 10000                      # 经验回放buffer大小
      discount_factor: 0.99                   # 折扣因子（未来奖励的权重）
      epsilon_decay: 0.995                    # ε衰减（从探索到利用的转换）

    step_sequence:
      # ========== 感知阶段 ==========
      # 步骤1：观察当前状态
      - name: "observe_state"
        reflection: modules.rl.observe
        args:
          environment: "env"
          session_id: "${session_id}"         # 会话ID（运行时动态）

      # ========== 决策阶段 ==========
      # 步骤2：根据状态选择动作（ε-greedy策略）
      - name: "select_action"
        reflection: modules.rl.select_action
        args:
          policy_network: "actor"             # 策略网络
          state: "last_result.state"          # 上一步得到的状态
          epsilon: "${epsilon}"               # 探索概率（运行时更新）

      # ========== 执行阶段 ==========
      # 步骤3：在环境中执行动作
      - name: "execute_action"
        reflection: modules.rl.execute_step
        args:
          environment: "env"
          action: "last_result.action"        # 选择的动作

      # ========== 反馈阶段 ==========
      # 步骤4：从环境获取反馈
      - name: "get_feedback"
        reflection: modules.rl.get_reward
        args:
          feedback: "last_result"             # 环境的执行结果

      # ========== 存储阶段 ==========
      # 步骤5：将经验存储到回放buffer（用于后续的批处理更新）
      - name: "store_experience"
        reflection: modules.rl.store_transition
        args:
          buffer: "replay_buffer"
          state: "observe_state.state"
          action: "select_action.action"
          reward: "get_feedback.reward"
          next_state: "get_feedback.next_state"
          done: "get_feedback.done"

      # ========== 学习阶段 ==========
      # 步骤6：计算TD-Error或Policy Gradient（损失）
      - name: "compute_td_error"
        reflection: modules.rl.compute_loss
        args:
          algorithm: "config.training_mode.subtype"  # 强化学习算法类型
          state: "observe_state.state"
          action: "select_action.action"
          reward: "get_feedback.reward"
          next_state: "get_feedback.next_state"
          actor: "actor"
          critic: "critic"

      # 步骤7：使用损失更新网络
      - name: "update_networks"
        reflection: modules.optimizer.apply_gradients
        args:
          optimizer: "actor_opt"
          loss: "last_result"
          model: "actor"

    # 循环条件
    loop_condition:
      check_type: "episode_based"             # 基于episode数量判断
      max_episodes: "config.training_pipeline.reinforcement.parameters.episodes"

    # 评估配置
    evaluation:
      frequency: "episode"                    # 每N个episode评估一次
      eval_episodes: 10

  # ========== 无监督学习（聚类）流程 ==========
  # 流程图：
  # while not converged:
  #   assign clusters
  #   update centroids
  #   check convergence
  #
  # 特点：不是epoch-batch结构，而是迭代直到收敛
  unsupervised_clustering:
    # 循环类型：迭代直到收敛（不是epoch-batch）
    loop_type: "iteration"

    parameters:
      max_iterations: 100                     # 最大迭代次数（防止死循环）
      n_clusters: 5                           # 簇的数量
      convergence_threshold: 1e-4             # 收敛阈值

    step_sequence:
      # 步骤1：加载所有数据
      - name: "load_data"
        reflection: modules.data.get_all_data
        args:
          loader: "unsupervised_loader"

      # 步骤2：分配样本到最近的簇
      - name: "assign_clusters"
        reflection: modules.clustering.assign
        args:
          data: "last_result"                 # 上一步的数据
          centroids: "${centroids}"           # 当前簇心（运行时维护）

      # 步骤3：根据分配更新簇心
      - name: "update_centroids"
        reflection: modules.clustering.update_centroids
        args:
          data: "last_result.data"
          assignments: "last_result.assignments"

      # 步骤4：检查是否收敛
      - name: "check_convergence"
        reflection: modules.clustering.check_convergence
        args:
          old_centroids: "${centroids}"
          new_centroids: "last_result"
          threshold: "config.training_pipeline.unsupervised_clustering.parameters.convergence_threshold"

    # 循环条件：收敛时停止
    loop_condition:
      check_type: "convergence_based"         # 基于收敛判断
      convergence_field: "last_result.converged"

  # ========== 自监督学习流程 ==========
  # 流程图：
  # for epoch in epochs:
  #   for batch in data:
  #     aug1 = augment(batch)
  #     aug2 = augment(batch)
  #     z1 = encoder(aug1)
  #     z2 = encoder(aug2)
  #     loss = contrastive_loss(z1, z2)  # z1和z2应该相似
  #     update(loss)
  #
  # 特点：无标签，但通过数据增强生成对比对
  self_supervised:
    loop_type: "epoch_batch"

    parameters:
      epochs: 100
      steps_per_epoch: null

    step_sequence:
      # 步骤1：获取batch数据
      - name: "fetch_batch"
        reflection: modules.data.get_batch
        args:
          loader: "self_supervised_loader"

      # 步骤2：数据增强（生成两个版本）
      - name: "augment_data"
        reflection: modules.augmentation.augment_pair
        args:
          data: "batch.x"
          transforms: "config.data_manager.self_supervised_source.augmentation.transforms"

      # 步骤3：编码第一个增强版本
      - name: "forward_view1"
        reflection: modules.inference.forward
        args:
          model: "encoder"
          inputs: "last_result.augmented_1"

      # 步骤4：编码第二个增强版本
      - name: "forward_view2"
        reflection: modules.inference.forward
        args:
          model: "encoder"
          inputs: "last_result.augmented_2"

      # 步骤5：计算对比损失（两个编码应该相似）
      - name: "compute_contrastive_loss"
        reflection: modules.losses.compute_contrastive_loss
        args:
          z1: "forward_view1"                 # 第一个编码
          z2: "forward_view2"                 # 第二个编码
          loss_fn: "contrastive_loss"

      # 步骤6：反向传播
      - name: "backward_pass"
        reflection: modules.optimizer.compute_gradients
        args:
          loss: "last_result"
          model: "encoder"

      # 步骤7：更新参数
      - name: "update_params"
        reflection: modules.optimizer.apply_gradients
        args:
          optimizer: "encoder_opt"
          gradients: "last_result"
          model: "encoder"

    loop_condition:
      check_type: "epoch_based"
      max_epochs: "config.training_pipeline.self_supervised.parameters.epochs"

  # ========== 无监督学习（自编码器）流程 ==========
  # 流程图：
  # for epoch in epochs:
  #   for batch in data:
  #     z = encoder(batch)
  #     recon = decoder(z)
  #     loss = ||recon - batch||²
  #     update(loss)
  unsupervised_autoencoder:
    loop_type: "epoch_batch"

    parameters:
      epochs: 100
      steps_per_epoch: null

    step_sequence:
      # 步骤1：获取batch
      - name: "fetch_batch"
        reflection: modules.data.get_batch
        args:
          loader: "unsupervised_loader"

      # 步骤2：编码
      - name: "forward_encoder"
        reflection: modules.inference.forward
        args:
          model: "encoder"
          inputs: "batch.x"

      # 步骤3：解码
      - name: "forward_decoder"
        reflection: modules.inference.forward
        args:
          model: "decoder"
          inputs: "last_result"

      # 步骤4：计算重构损失
      - name: "compute_reconstruction_loss"
        reflection: modules.losses.compute_reconstruction_loss
        args:
          reconstructed: "last_result"        # 解码的结果
          original: "batch.x"                 # 原始数据
          loss_fn: "reconstruction_loss"

      # 步骤5：反向传播（对编码器和解码器都计算梯度）
      - name: "backward_pass"
        reflection: modules.optimizer.compute_gradients
        args:
          loss: "last_result"
          models: ["encoder", "decoder"]      # 注意：可以指定多个模型

      # 步骤6：更新参数
      - name: "update_params"
        reflection: modules.optimizer.apply_gradients
        args:
          optimizer: "encoder_opt"
          gradients: "last_result"
          models: ["encoder", "decoder"]

    loop_condition:
      check_type: "epoch_based"
      max_epochs: "config.training_pipeline.unsupervised_autoencoder.parameters.epochs"

  # ========== 多任务学习流程 ==========
  # 流程图：
  # for epoch in epochs:
  #   for batch in data:
  #     shared_feat = shared_encoder(batch)
  #     out1 = task1_head(shared_feat)
  #     out2 = task2_head(shared_feat)
  #     out3 = task3_head(shared_feat)
  #     loss1 = criterion1(out1, target1)
  #     loss2 = criterion2(out2, target2)
  #     loss3 = criterion3(out3, target3)
  #     total_loss = w1*loss1 + w2*loss2 + w3*loss3
  #     update(total_loss)
  multi_task:
    loop_type: "epoch_batch"

    parameters:
      epochs: 100
      steps_per_epoch: null
      # 各任务的损失权重（用于加权平均）
      task_weights:
        task1: 0.5                            # 任务1的权重
        task2: 0.3                            # 任务2的权重
        task3: 0.2                            # 任务3的权重

    step_sequence:
      # 步骤1：获取batch
      - name: "fetch_batch"
        reflection: modules.data.get_batch
        args:
          loader: "train_loader"

      # 步骤2：共享编码器（所有任务共用的特征提取）
      - name: "forward_shared_encoder"
        reflection: modules.inference.forward
        args:
          model: "shared_encoder"
          inputs: "batch.x"

      # 步骤3：任务1的特殊头
      - name: "forward_task1_head"
        reflection: modules.inference.forward
        args:
          model: "task1_head"
          inputs: "last_result"

      # 步骤4：任务2的特殊头
      - name: "forward_task2_head"
        reflection: modules.inference.forward
        args:
          model: "task2_head"
          inputs: "last_result"

      # 步骤5：任务3的特殊头
      - name: "forward_task3_head"
        reflection: modules.inference.forward
        args:
          model: "task3_head"
          inputs: "last_result"

      # 步骤6：计算所有任务的损失并加权平均
      - name: "compute_task_losses"
        reflection: modules.losses.compute_multi_task_losses
        args:
          # 三个任务的预测结果
          predictions: {
            task1: "forward_task1_head",
            task2: "forward_task2_head",
            task3: "forward_task3_head"
          }
          # 三个任务的目标标签
          targets: {
            task1: "batch.y1",
            task2: "batch.y2",
            task3: "batch.y3"
          }
          # 权重配置（来自parameters）
          weights: "config.training_pipeline.multi_task.parameters.task_weights"

      # 步骤7：反向传播（对所有模型计算梯度）
      - name: "backward_pass"
        reflection: modules.optimizer.compute_gradients
        args:
          loss: "last_result.total_loss"      # 加权的总损失
          models: ["shared_encoder", "task1_head", "task2_head", "task3_head"]

      # 步骤8：更新所有模型的参数
      - name: "update_params"
        reflection: modules.optimizer.apply_gradients
        args:
          optimizer: "shared_opt"             # 通常共享一个优化器
          gradients: "last_result"
          models: ["shared_encoder", "task1_head", "task2_head", "task3_head"]

    loop_condition:
      check_type: "epoch_based"
      max_epochs: "config.training_pipeline.multi_task.parameters.epochs"

# ========== 第八层：奖励函数定义 ==========
# 作用：定义强化学习中的奖励计算方式
# 说明：仅在强化学习中使用，用于将环境反馈转化为学习信号
# 重要：不同的游戏或任务可以使用不同的奖励函数，无需修改代码
reward_functions:
  # 类型1：简单表达式
  # 用于直接计算奖励（如策略梯度）
  simple_reward:
    type: "expression"                       # 类型：表达式
    formula: "reward * log_prob"             # 公式：奖励乘以log概率
    # 公式中使用的变量
    variables:
      - name: "reward"                       # 变量名
        source: "feedback.reward"            # 数据来源
      - name: "log_prob"
        source: "action_prob"

  # 类型2：复杂计算（调用脚本）
  # 用于TD-learning或其他复杂算法
  td_target:
    type: "script"                           # 类型：脚本函数
    function: "modules.rewards.compute_td_target"  # 函数路径
    args:
      gamma: 0.99                            # 折扣因子

  # 类型3：多阶段评分
  # 用于在episode结束时统一计分（如游戏的最终胜负）
  game_score:
    type: "aggregation"                      # 类型：聚合
    trigger: "done == true"                  # 触发条件：episode结束
    formula: |
      base_score = final_reward
      win_bonus = 100 if victory else 0
      total = base_score + win_bonus

# ========== 第九层：评估流程 ==========
# 作用：定义模型在验证/测试集上的评估方式
# 说明：不同的训练方式有完全不同的评估指标
evaluation:
  # 监督学习的评估
  # 在验证集上评估分类精度、F1分数等
  supervised_eval:
    frequency: "epoch"                       # 评估频率（每epoch或每step）
    split: "val"                             # 使用哪个数据集（train/val/test）

    # 评估的步骤
    steps:
      # 步骤1：前向传播
      - name: "forward_pass"
        reflection: modules.inference.forward
        args:
          model: "generator"
          inputs: "batch.x"

      # 步骤2：计算评估指标
      - name: "compute_metrics"
        reflection: modules.metrics.compute_metrics
        args:
          predictions: "last_result"
          targets: "batch.y"
          # 计算哪些指标
          metrics: ["accuracy", "f1", "precision"]

  # 强化学习的评估
  # 运行多个episode，统计平均奖励和胜率
  rl_eval:
    frequency: "episode"                     # 每N个episode评估一次
    eval_episodes: 10                        # 评估用的episode数

    steps:
      # 步骤1：运行一个完整的episode（不更新模型）
      - name: "episode_rollout"
        reflection: modules.rl.eval_episode
        args:
          policy: "actor"                    # 策略网络
          environment: "env"

      # 步骤2：计算总奖励
      - name: "compute_reward"
        reflection: modules.metrics.compute_total_reward
        args:
          trajectory: "last_result"          # episode的轨迹

# ========== 第十层：导出配置 ==========
# 作用：定义如何导出训练好的模型
# 说明：将PyTorch/TensorFlow模型转换为ONNX格式
#       ONNX格式可以在任何支持ONNX的框架中加载（跨平台兼容性）
export:
  reflection: torch.onnx.export             # 使用PyTorch的ONNX导出函数
  args:
    model: "generator"                      # 要导出的模型名称
    args: [[1, 100]]                        # 模型的dummy input（用于追踪计算图）
    f: "outputs/onnx/model.onnx"           # 输出文件路径
    opset_version: 16                       # ONNX操作集版本

# ========== 第十一层：部署配置 ==========
# 作用：定义模型部署为服务的方式
# 说明：将训练好的模型包装为HTTP服务
deployment:
  host: "127.0.0.1"                         # 服务绑定的主机
  port: 9000                                # 服务绑定的端口
  model_key: "generator"                    # 加载哪个模型
  service_type: "onnx_inference"            # 服务类型（ONNX推理服务）

# ============================================================
# 配置使用说明
# ============================================================
#
# 1. 选择训练方式：
#    - 修改 training_mode.type 字段选择训练方式
#    - 对应的 data_manager 和 training_pipeline 会自动生效
#
# 2. 组织工作流：
#    - Python主循环读取此配置
#    - 根据 training_mode.type 选择对应的流程
#    - 通过反射调用 step_sequence 中的各个步骤
#
# 3. 参数绑定：
#    - "last_result" ：上一步的输出
#    - "${variable}" ：运行时变量（如epsilon、session_id）
#    - "config.path.to.value" ：配置中的其他值
#    - "batch.x" ：当前batch的字段
#
# 4. 扩展新训练方式：
#    - 在 training_pipeline 中添加新的方式
#    - 定义 loop_type 和 step_sequence
#    - Python代码会自动支持（无需修改！）
#
# 5. 调试技巧：
#    - 检查每个step的输入参数是否正确
#    - 使用日志打印 last_result 的内容
#    - 验证模型和数据加载器是否初始化成功
#
# ============================================================
