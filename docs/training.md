# å®Œæ•´çš„æœºå™¨å­¦ä¹ è®­ç»ƒæ–¹å¼åˆ†ç±»ä½“ç³»

> **é‡è¦å‰æ**ï¼šæ·±åº¦å­¦ä¹  vs æµ…å±‚å­¦ä¹ åªå½±å“**æ¨¡å‹æ„ç­‘**ï¼Œä¸å½±å“**è®­ç»ƒæµç¨‹**ã€‚
> è®­ç»ƒæ–¹å¼ç”±**æ•°æ®æ¥æºã€æ ‡ç­¾æ¥æºã€åé¦ˆæœºåˆ¶**å†³å®šã€‚

---

## ä¸€ã€è®­ç»ƒæ–¹å¼çš„æ ¹æœ¬åˆ†ç±»ç»´åº¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æœºå™¨å­¦ä¹ è®­ç»ƒæ–¹å¼                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æŒ‰ç›‘ç£ç¨‹åº¦ï¼šæœ‰ç›‘ç£ â†” æ— ç›‘ç£ â†” åŠç›‘ç£  â”‚
â”‚ æŒ‰åé¦ˆæ–¹å¼ï¼šæ ‡ç­¾ â†” å¥–åŠ± â†” è‡ªç”Ÿæˆ       â”‚
â”‚ æŒ‰æ•°æ®æµï¼šç¦»çº¿ â†” åœ¨çº¿ â†” æµå¼           â”‚
â”‚ æŒ‰å­¦ä¹ ç›®æ ‡ï¼šé¢„æµ‹ â†” å†³ç­– â†” è¡¨ç¤º         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€ä¸»è¦è®­ç»ƒæ–¹å¼è¯¦ç»†åˆ—è¡¨

### 1. ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰

#### ç‰¹å¾
- âœ… æ•°æ®+æ ‡ç­¾é…å¯¹ï¼ˆX, Yï¼‰
- âœ… æ ‡ç­¾é¢„å…ˆå·²çŸ¥/äººå·¥æ ‡æ³¨
- âœ… ç¦»çº¿å­¦ä¹ 
- âœ… å³æ—¶åé¦ˆ

#### è®­ç»ƒæµç¨‹
```
åŠ è½½æ‰¹æ¬¡(X, Y) â†’ Forward â†’ è®¡ç®—Loss(Y_pred, Y) â†’ Backward â†’ æ›´æ–°å‚æ•°
```

#### å¸¸è§ç®—æ³•
- åˆ†ç±»ï¼šé€»è¾‘å›å½’ã€SVMã€å†³ç­–æ ‘ã€éšæœºæ£®æ—
- å›å½’ï¼šçº¿æ€§å›å½’ã€å²­å›å½’
- ç¥ç»ç½‘ç»œï¼šMLPã€CNNã€RNN

#### é…ç½®ç¤ºä¾‹
```yaml
training:
  training_type: supervised
  data:
    loader: csv_loader
    has_labels: true
  steps:
    - compute_loss
    - backprop
```

#### ä»£ç 
```python
for epoch in epochs:
    for X_batch, y_batch in train_loader:
        y_pred = model(X_batch)
        loss = loss_fn(y_pred, y_batch)  # â† æ ‡ç­¾å·²çŸ¥
        loss.backward()
        optimizer.step()
```

---

### 2. æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰

#### ç‰¹å¾
- âŒ æ— æ ‡ç­¾æ•°æ®
- âœ… è‡ªç”Ÿæˆç›®æ ‡
- âœ… ç¦»çº¿å­¦ä¹ 
- âœ… å†…éƒ¨åé¦ˆï¼ˆå¦‚é‡æ„è¯¯å·®ï¼‰

#### å­ç±»å‹

##### 2.1 èšç±»ï¼ˆClusteringï¼‰
```
æ— æ ‡ç­¾æ•°æ® â†’ æ¨¡å‹ â†’ è‡ªåŠ¨åˆ†ç»„
```

**å¸¸è§ç®—æ³•**ï¼šK-meansã€GMMã€DBSCAN

**æµç¨‹**ï¼š
```
åˆå§‹åŒ–ç°‡å¿ƒ â†’ åˆ†é…æ ·æœ¬ â†’ é‡æ–°è®¡ç®—ç°‡å¿ƒ â†’ æ”¶æ•›
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: unsupervised_clustering
  algorithm: kmeans
  params:
    n_clusters: 5
  steps:
    - assign_clusters
    - update_centroids
```

##### 2.2 é™ç»´ï¼ˆDimensionality Reductionï¼‰
```
é«˜ç»´æ•°æ® â†’ æ¨¡å‹ â†’ ä½ç»´è¡¨ç¤º
```

**å¸¸è§ç®—æ³•**ï¼šPCAã€t-SNEã€è‡ªç¼–ç å™¨

**æµç¨‹ï¼ˆä»¥è‡ªç¼–ç å™¨ä¸ºä¾‹ï¼‰**ï¼š
```
æ•°æ® â†’ ç¼–ç å™¨ â†’ ä½ç»´å‘é‡ â†’ è§£ç å™¨ â†’ é‡æ„æ•°æ®
      â† æœ€å°åŒ–é‡æ„è¯¯å·® â†
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: unsupervised_autoencoder
  model:
    encoder:
      layers: [input:784 â†’ 128 â†’ 64]
    decoder:
      layers: [64 â†’ 128 â†’ 784]
  steps:
    - forward_encoder_decoder
    - compute_reconstruction_loss
    - backprop
```

**ä»£ç **ï¼š
```python
for epoch in epochs:
    for X_batch in data_loader:  # â† æ— æ ‡ç­¾
        encoded = encoder(X_batch)
        decoded = decoder(encoded)
        loss = mse(decoded, X_batch)  # è‡ªç”Ÿæˆç›®æ ‡ï¼šé‡æ„
        loss.backward()
        optimizer.step()
```

---

### 3. å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰

#### ç‰¹å¾
- âŒ æ— é¢„æ ‡ç­¾
- âœ… å¥–åŠ±åé¦ˆ
- âœ… äº¤äº’å¼å­¦ä¹ 
- âœ… å»¶è¿Ÿåé¦ˆ

#### è®­ç»ƒæµç¨‹
```
Agentè§‚å¯ŸçŠ¶æ€ â†’ é€‰æ‹©åŠ¨ä½œ â†’ ç¯å¢ƒè¿”å›(å¥–åŠ±, æ–°çŠ¶æ€)
      â† Agentæ ¹æ®å¥–åŠ±æ›´æ–°ç­–ç•¥ â†
```

#### ä¸»è¦ç®—æ³•

##### 3.1 å€¼æ–¹æ³•ï¼ˆValue-Basedï¼‰
ç›®æ ‡ï¼šå­¦ä¹ æœ€ä¼˜ä»·å€¼å‡½æ•° V(s) æˆ– Q(s,a)

```
Q-learning: Q(s,a) = Q(s,a) + Î±[r + Î³*max_a'Q(s',a') - Q(s,a)]
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: reinforcement_learning
  algorithm: q_learning
  params:
    discount_factor: 0.99
    learning_rate: 0.01
  steps:
    - observe_state
    - select_action_epsilon_greedy
    - execute_action
    - get_reward_next_state
    - compute_td_error
    - update_q_table
```

##### 3.2 ç­–ç•¥æ–¹æ³•ï¼ˆPolicy-Basedï¼‰
ç›®æ ‡ï¼šç›´æ¥ä¼˜åŒ–ç­–ç•¥ Ï€(a|s)

```
Policy Gradient: âˆ‡J(Î¸) = âˆ‡log Ï€(a|s) * R(Ï„)
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: reinforcement_learning
  algorithm: policy_gradient
  steps:
    - observe_state
    - forward_policy_network
    - sample_action_from_distribution
    - execute_action
    - get_reward_trajectory
    - compute_policy_gradient_loss
    - backprop
```

##### 3.3 è¡ŒåŠ¨è€…-è¯„è®ºå®¶æ–¹æ³•ï¼ˆActor-Criticï¼‰
ç»“åˆå€¼æ–¹æ³•å’Œç­–ç•¥æ–¹æ³•

```
Actorï¼ˆç­–ç•¥ï¼‰ï¼šé€‰æ‹©åŠ¨ä½œ
Criticï¼ˆä»·å€¼ï¼‰ï¼šè¯„ä¼°åŠ¨ä½œå¥½å
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: reinforcement_learning
  algorithm: actor_critic
  model:
    actor: policy_network
    critic: value_network
  steps:
    - observe_state
    - actor_predict_action
    - execute_action
    - critic_evaluate_value
    - compute_advantage: reward + Î³*V(s') - V(s)
    - update_actor: policy_gradient * advantage
    - update_critic: mse(advantage)
```

**ä»£ç **ï¼š
```python
for episode in episodes:
    state = env.reset()
    for step in max_steps:
        action = actor(state)  # ç­–ç•¥ç½‘ç»œ
        next_state, reward, done = env.step(action)

        value = critic(state)  # ä»·å€¼ç½‘ç»œ
        next_value = critic(next_state)
        advantage = reward + Î³*next_value - value

        actor_loss = -log_prob(action) * advantage
        critic_loss = advantage**2

        backprop(actor_loss + critic_loss)
```

---

### 4. è‡ªç›‘ç£å­¦ä¹ ï¼ˆSelf-Supervised Learningï¼‰

#### ç‰¹å¾
- âŒ æ— äººå·¥æ ‡ç­¾
- âœ… ä»æ•°æ®æœ¬èº«ç”Ÿæˆæ ‡ç­¾
- âœ… ç¦»çº¿å­¦ä¹ 
- âœ… å³æ—¶åé¦ˆ

#### å¸¸è§æ–¹æ³•

##### 4.1 å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰
```
åŒä¸€æ ·æœ¬çš„ä¸¤ä¸ªå¢å¼ºè§†å›¾ â†’ æ¨¡å‹ â†’ åº”è¯¥ç›¸è¿‘
ä¸åŒæ ·æœ¬çš„å¢å¼ºè§†å›¾    â†’ æ¨¡å‹ â†’ åº”è¯¥ç›¸è¿œ
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: self_supervised_contrastive
  augmentation:
    - random_crop
    - random_flip
    - random_color_jitter
  steps:
    - augment_data_twice
    - forward_both_views
    - compute_contrastive_loss
    - backprop
```

**ä»£ç **ï¼š
```python
for batch in data_loader:
    # åŒä¸€æ ·æœ¬çš„ä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬ï¼ˆè‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾æ¦‚å¿µï¼‰
    x_i = augment(batch)
    x_j = augment(batch)

    z_i = encoder(x_i)
    z_j = encoder(x_j)

    # å¯¹æ¯”æŸå¤±ï¼šç›¸ä¼¼åº¦æœ€å¤§åŒ–
    loss = contrastive_loss(z_i, z_j)
    loss.backward()
```

##### 4.2 æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼Œå¦‚BERTï¼‰
```
æ–‡æœ¬ "æˆ‘çˆ±[MASK]å­¦ä¹ " â†’ æ¨¡å‹ â†’ é¢„æµ‹[MASK]=æ·±åº¦
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: self_supervised_mlm
  masking_ratio: 0.15
  steps:
    - mask_tokens
    - forward_model
    - compute_mlm_loss
    - backprop
```

##### 4.3 è‡ªç¼–ç å˜åˆ†ï¼ˆVAEï¼‰
```
æ•°æ® â†’ ç¼–ç ä¸ºåˆ†å¸ƒ â†’ é‡‡æ · â†’ è§£ç  â†’ é‡æ„
      â† æœ€å°åŒ–é‡æ„è¯¯å·® + KLæ•£åº¦ â†
```

---

### 5. åŠç›‘ç£å­¦ä¹ ï¼ˆSemi-Supervised Learningï¼‰

#### ç‰¹å¾
- ğŸŸ¡ éƒ¨åˆ†æ ‡æ³¨ã€éƒ¨åˆ†æ— æ ‡ç­¾
- âœ… ç»“åˆç›‘ç£+æ— ç›‘ç£
- âœ… ç¦»çº¿å­¦ä¹ 

#### å¸¸è§æ–¹æ³•

##### 5.1 ä¼ªæ ‡ç­¾ï¼ˆPseudo-Labelingï¼‰
```
æœ‰æ ‡ç­¾æ•°æ® â†’ è®­ç»ƒ â†’ æ¨¡å‹
æ— æ ‡ç­¾æ•°æ® â†’ æ¨¡å‹é¢„æµ‹ â†’ ä¼ªæ ‡ç­¾ â†’ ä½œä¸ºæ ‡ç­¾é‡æ–°è®­ç»ƒ
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: semi_supervised_pseudo_labeling
  labeled_ratio: 0.1
  confidence_threshold: 0.9
  steps:
    - train_on_labeled
    - predict_on_unlabeled
    - filter_high_confidence
    - add_pseudo_labels
    - retrain_on_all
```

**ä»£ç **ï¼š
```python
labeled_loader, unlabeled_loader = split_data()

for epoch in epochs:
    # ç¬¬1æ­¥ï¼šåœ¨æœ‰æ ‡ç­¾æ•°æ®ä¸Šè®­ç»ƒ
    for X, y in labeled_loader:
        y_pred = model(X)
        loss = cross_entropy(y_pred, y)
        loss.backward()

    # ç¬¬2æ­¥ï¼šåœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šç”Ÿæˆä¼ªæ ‡ç­¾
    pseudo_labels = []
    for X_unlabeled in unlabeled_loader:
        y_pred = model(X_unlabeled)
        if max(y_pred) > confidence_threshold:
            pseudo_labels.append(argmax(y_pred))

    # ç¬¬3æ­¥ï¼šæ··åˆè®­ç»ƒ
    for X, y_pseudo in zip(unlabeled_loader, pseudo_labels):
        y_pred = model(X)
        loss = cross_entropy(y_pred, y_pseudo)
        loss.backward()
```

##### 5.2 ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆConsistency Regularizationï¼‰
```
æœ‰æ ‡ç­¾æ•°æ® â†’ æ ‡å‡†ç›‘ç£æŸå¤±
æ— æ ‡ç­¾æ•°æ® â†’ ä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬åº”é¢„æµ‹ä¸€è‡´
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: semi_supervised_consistency
  steps:
    - supervised_loss_labeled
    - consistency_loss_unlabeled
    - combine: Î»1*sup_loss + Î»2*cons_loss
```

---

### 6. å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMulti-Task Learningï¼‰

#### ç‰¹å¾
- âœ… åŒæ—¶å­¦ä¹ å¤šä¸ªç›¸å…³ä»»åŠ¡
- âœ… å…±äº«è¡¨ç¤ºï¼Œä»»åŠ¡ç‰¹å®šå¤´
- âœ… ç¦»çº¿å­¦ä¹ 

#### è®­ç»ƒæµç¨‹
```
å…±äº«ç¼–ç å™¨ â†’ Task1å¤´ â†’ Loss1
         â†’ Task2å¤´ â†’ Loss2
         â†’ Task3å¤´ â†’ Loss3
æ€»Loss = Î±1*Loss1 + Î±2*Loss2 + Î±3*Loss3
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: multi_task
  tasks:
    - name: task_classification
      weight: 0.5
    - name: task_segmentation
      weight: 0.3
    - name: task_depth_estimation
      weight: 0.2
  steps:
    - forward_shared_encoder
    - forward_task1_head
    - forward_task2_head
    - forward_task3_head
    - compute_combined_loss
    - backprop
```

---

### 7. è¿ç§»å­¦ä¹ ï¼ˆTransfer Learningï¼‰

#### ç‰¹å¾
- âœ… å…ˆåœ¨æºä»»åŠ¡é¢„è®­ç»ƒ
- âœ… å†åœ¨ç›®æ ‡ä»»åŠ¡å¾®è°ƒ
- âœ… ç¦»çº¿å­¦ä¹ 

#### è®­ç»ƒæµç¨‹
```
é˜¶æ®µ1ï¼šæºä»»åŠ¡ï¼ˆå¤§æ•°æ®é›†ï¼‰
ImageNet â†’ è®­ç»ƒ â†’ é¢„è®­ç»ƒæƒé‡

é˜¶æ®µ2ï¼šç›®æ ‡ä»»åŠ¡ï¼ˆå°æ•°æ®é›†ï¼‰
åŒ»å­¦å›¾åƒ â†’ åŠ è½½é¢„è®­ç»ƒæƒé‡ â†’ å¾®è°ƒ
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: transfer_learning
  stages:
    - name: pretraining
      dataset: imagenet
      epochs: 100
      freeze_layers: none

    - name: fine_tuning
      dataset: medical_images
      epochs: 20
      freeze_layers: [0, 1, 2]  # å†»ç»“å‰3å±‚
      learning_rate: 0.0001
```

---

### 8. å…ƒå­¦ä¹ ï¼ˆMeta-Learningï¼‰

#### ç‰¹å¾
- âœ… å­¦ä¹ å¦‚ä½•å­¦ä¹ 
- âœ… å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
- âœ… å°æ ·æœ¬å­¦ä¹ 

#### å¸¸è§ç®—æ³•ï¼šMAMLã€Prototypical Networks

**é…ç½®**ï¼š
```yaml
training:
  training_type: meta_learning
  algorithm: maml
  params:
    inner_lr: 0.01
    outer_lr: 0.001
    inner_steps: 5
  steps:
    - sample_task
    - inner_loop_optimize
    - outer_loop_optimize
```

---

### 9. åœ¨çº¿å­¦ä¹ ï¼ˆOnline Learningï¼‰

#### ç‰¹å¾
- âœ… æ•°æ®æµå¼åˆ°è¾¾
- âœ… æ¯æ¬¡1æ¡æˆ–å°æ‰¹é‡
- âœ… ç«‹å³æ›´æ–°
- âœ… é€‚åº”éå¹³ç¨³åˆ†å¸ƒ

#### è®­ç»ƒæµç¨‹
```
å¯¹äºæ¯æ¡æ•°æ®ï¼š
  é¢„æµ‹ â†’ è§‚å¯Ÿæ ‡ç­¾ â†’ æ›´æ–° â†’ ä¸‹ä¸€æ¡æ•°æ®
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: online_learning
  data:
    stream: true
    batch_size: 1
  steps:
    - fetch_single_instance
    - predict
    - get_label
    - update_model
```

**ä»£ç **ï¼š
```python
for data_point in streaming_data:
    y_pred = model(data_point)
    loss = loss_fn(y_pred, true_label)
    loss.backward()
    optimizer.step()  # â† æ¯æ¡æ•°æ®éƒ½æ›´æ–°
```

---

### 10. ä¸»åŠ¨å­¦ä¹ ï¼ˆActive Learningï¼‰

#### ç‰¹å¾
- ğŸŸ¡ åŠç›‘ç£
- âœ… æ¨¡å‹ä¸»åŠ¨é€‰æ‹©æ ‡æ³¨å“ªäº›æ ·æœ¬
- âœ… å‡å°‘æ ‡æ³¨æˆæœ¬

#### è®­ç»ƒæµç¨‹
```
æ¨¡å‹ â†’ è¯„ä¼°ä¸ç¡®å®šæ€§ â†’ é€‰æ‹©æœ€éš¾æ ·æœ¬ â†’ äººå·¥æ ‡æ³¨
   â† é‡æ–°è®­ç»ƒ â†
```

**é…ç½®**ï¼š
```yaml
training:
  training_type: active_learning
  selection_strategy: uncertainty_sampling
  params:
    initial_labeled: 0.05
    query_size: 100
  steps:
    - train_on_labeled
    - evaluate_uncertainty_unlabeled
    - query_top_uncertain
    - get_labels_human
    - retrain
```

---

### 11. å¯¹æŠ—è®­ç»ƒï¼ˆAdversarial Trainingï¼‰

#### ç‰¹å¾
- âœ… ä¸¤ä¸ªç½‘ç»œå¯¹æŠ—
- âœ… ç”Ÿæˆå™¨ vs åˆ¤åˆ«å™¨
- âœ… ç¦»çº¿å­¦ä¹ 

#### å¸¸è§ç®—æ³•ï¼šGAN

**é…ç½®**ï¼š
```yaml
training:
  training_type: adversarial
  algorithm: gan
  model:
    generator: gen_network
    discriminator: disc_network
  steps:
    - sample_noise
    - generator_forward
    - discriminator_real
    - discriminator_fake
    - compute_gen_loss
    - compute_disc_loss
    - update_generator
    - update_discriminator
```

---

### 12. è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰

#### ç‰¹å¾
- âœ… æŒ‰éš¾åº¦é€’è¿›å­¦ä¹ 
- âœ… ä»æ˜“åˆ°éš¾
- âœ… æå‡è®­ç»ƒç¨³å®šæ€§

**é…ç½®**ï¼š
```yaml
training:
  training_type: supervised_curriculum
  curriculum:
    - stage: easy
      epochs: 10
      data_filter: difficulty < 0.3
    - stage: medium
      epochs: 15
      data_filter: 0.3 <= difficulty < 0.7
    - stage: hard
      epochs: 20
      data_filter: difficulty >= 0.7
  steps:
    - load_curriculum_stage
    - train_supervised
```

---

## ä¸‰ã€è®­ç»ƒæ–¹å¼å¯¹æ¯”çŸ©é˜µ

| æ–¹å¼ | æ ‡ç­¾éœ€æ±‚ | æ•°æ®æµ | åé¦ˆ | äº¤äº’ | å…¸å‹åº”ç”¨ |
|-----|--------|--------|------|------|--------|
| **ç›‘ç£å­¦ä¹ ** | âœ…å¿…éœ€ | ç¦»çº¿ | å³æ—¶æ ‡ç­¾ | âŒ | åˆ†ç±»ã€å›å½’ |
| **æ— ç›‘ç£å­¦ä¹ ** | âŒæ—  | ç¦»çº¿ | å†…éƒ¨æŸå¤± | âŒ | èšç±»ã€é™ç»´ |
| **å¼ºåŒ–å­¦ä¹ ** | âŒæ—  | äº¤äº’ | å»¶è¿Ÿå¥–åŠ± | âœ… | æ¸¸æˆã€æœºå™¨äºº |
| **è‡ªç›‘ç£å­¦ä¹ ** | âŒæ—  | ç¦»çº¿ | è‡ªç”Ÿæˆç›®æ ‡ | âŒ | é¢„è®­ç»ƒã€è¡¨ç¤ºå­¦ä¹  |
| **åŠç›‘ç£å­¦ä¹ ** | ğŸŸ¡éƒ¨åˆ† | ç¦»çº¿ | æ··åˆ | âŒ | æ ‡æ³¨ç¨€ç¼ºåœºæ™¯ |
| **å¤šä»»åŠ¡å­¦ä¹ ** | âœ…å¿…éœ€ | ç¦»çº¿ | å¤šç›®æ ‡ | âŒ | å¤šä¸ªç›¸å…³ä»»åŠ¡ |
| **è¿ç§»å­¦ä¹ ** | ğŸŸ¡éƒ¨åˆ† | ç¦»çº¿ | æ··åˆ | âŒ | å°æ ·æœ¬å­¦ä¹  |
| **å…ƒå­¦ä¹ ** | ğŸŸ¡éƒ¨åˆ† | ç¦»çº¿ | æ¢¯åº¦ | âŒ | å¿«é€Ÿé€‚åº” |
| **åœ¨çº¿å­¦ä¹ ** | âœ…å¿…éœ€ | æµå¼ | å³æ—¶æ ‡ç­¾ | âŒ | æµå¼æ•°æ® |
| **ä¸»åŠ¨å­¦ä¹ ** | ğŸŸ¡éƒ¨åˆ† | æ··åˆ | å³æ—¶æ ‡ç­¾ | âœ… | æˆæœ¬ä¼˜åŒ– |
| **å¯¹æŠ—è®­ç»ƒ** | ğŸŸ¡éƒ¨åˆ† | ç¦»çº¿ | å¯¹æŠ— | âŒ | ç”Ÿæˆä»»åŠ¡ |
| **è¯¾ç¨‹å­¦ä¹ ** | âœ…å¿…éœ€ | ç¦»çº¿ | éš¾åº¦é€’è¿› | âŒ | éš¾ä»»åŠ¡å­¦ä¹  |

---

## å››ã€é’ˆå¯¹ä½ çš„é¡¹ç›®çš„å»ºè®®

æ ¹æ®ä½ çš„æè¿°ï¼š"æ¯æ¬¡è¯·æ±‚ä»…èƒ½è·å–ä¸€æ¡æ•°æ®ï¼ŒæŸ¥çœ‹æ‰§è¡Œç»“æœè¿˜éœ€è¦ä¸€æ¬¡è¯·æ±‚"

**è¿™æ˜¯å…¸å‹çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ **ï¼Œä½†ä¹Ÿå¯ä»¥ç»„åˆå…¶ä»–æ–¹å¼ï¼š

```yaml
training:
  training_type: "online_reinforcement_learning"
  # å¯é€‰ç»“åˆï¼šå¯¹æŠ—è®­ç»ƒï¼ˆGANï¼‰
  # å¯é€‰ç»“åˆï¼šè¯¾ç¨‹å­¦ä¹ ï¼ˆä»ç®€å•æ¸¸æˆåˆ°å¤æ‚ï¼‰
  # å¯é€‰ç»“åˆï¼šä¸»åŠ¨å­¦ä¹ ï¼ˆé€‰æ‹©æœ€éš¾æ ·æœ¬ï¼‰
```

å…³é”®æ˜¯ä½ çš„é…ç½®æ¶æ„æ”¯æŒ**ä»»æ„è®­ç»ƒæ–¹å¼**ï¼Œå› ä¸ºï¼š
- âœ… æ¨¡å‹æ„ç­‘ï¼ˆæ·±åº¦/æµ…å±‚ï¼‰ç‹¬ç«‹
- âœ… æ•°æ®æµã€å¥–åŠ±ã€ç›®æ ‡é€šè¿‡é…ç½®å®šä¹‰
- âœ… Pythonä»£ç åªéœ€å®ç°ä¸åŒçš„ä¸»å¾ªç¯é€»è¾‘
